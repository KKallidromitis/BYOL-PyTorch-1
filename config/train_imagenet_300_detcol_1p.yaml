model:
  base_momentum: 0.99
  masknet: false
  backbone:
    type: "resnet50"
    pretrained: false
  projection:   # src: A.2 of detcon paper
    type: "MLP" 
    input_dim: 2048
    hidden_dim: 4096
    output_dim: 256
  predictor:
    type: "MLP"
    input_dim: 256
    hidden_dim: 4096
    output_dim: 256

clustering:
  per_image: false
  gather: false
  no_slic: true
  use_pca: 0 # set to 0 or false if disabled. otherwise num of principal components. 
  # WARNING: use gather or add_views without PCA may consume significant amount of memory
  add_views: true #only works without slic
  weight_color: 0.0
  weight_spatial: 0.0
  superpixel: slic # switch SLIC vs FH
  batch_size: 0
  scheduler:
    type: 'linear'
    epochs: [0,300]
    values: [9999,9999]
    # config = {
    #     "type":"cosine",
    #     "epochs":[0,300],
    #     "values":[64,1] # [1,64] work as well
    # }
    # # config = {
    # #     "type":"piecewise",
    # #     "epochs":[0,150,300],
    # #     "values":[1,64,32] # 0-150:1 ,150-300:64,300-inf:32
    # # }
    # # config = {
    # #     "type":"linear",
    # #     "epochs":[0,150,300],
    # #     "values":[1,64,32]
    # # }
amp:
  sync_bn: True
  opt_level: "O0"

data:
  image_dir: "./imagenet" #TODO: Change to match Japan Cluster
  subset: "imagenet100" #Set to imagenet100 for imagenet100 or imagenet1p for 1p. "" = full imagenet
  mask_type: ""
  resize_size: 224 # src: 3.1
  data_workers: 8
  overlap_indicator: true 
  weight: false
  n_kmeans: 9999
  slic_segments: 100
  train_batch_size: 512 # src: A.3 (Global should be 4096 = batch_size x num_gpu)
  val_batch_size: 32 #  Should not matter
  dual_views: true
  over_lap_mask: false
  num_examples: 1281167 #For imagenet1p, imagenet100, byol_trainer automatically updates num_examples accordingly

optimizer:
  type: lars
  base_lr: 0.3
  lr_type: "cosine"
  momentum: 0.9 # src: Deepmind code config
  weight_decay: 1.0e-6
  total_epochs: 300
  warmup_epochs: 3 # src: Deepmind code; should be 1/100 of total epoches
  exclude_bias_and_bn: true
  legacy_schedule: false # use old scheduler defined by lr_type
  scheduler: # a scheduler definition, epochs should be scaled to [0,100], values is a factor multiplied on base_lr
    type: 'cosine'
    epochs: [0,100]
    values: [1.0,0]

loss: #src: 3.1
  temperature: 0.1
  mask_rois: 16
  type: byol # new loss

checkpoint:
  time_stamp:
  resume_path:
  save_epoch: 50
  ckpt_path: "/shared/jacklishufan/ckpt/detconb/{}/{}_{}_{}.pth.tar"

log:
  log_step: 10
  log_dir:
  log_all: False
  enable_wandb: true

stage: "train"
distributed: True
seed: 0
