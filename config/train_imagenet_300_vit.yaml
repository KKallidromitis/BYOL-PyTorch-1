model:
  base_momentum: 0.99
  masknet: false
  backbone:
    type: "vit"
    pretrained: false
    feature_resolution: 14 # 14 x 14
  projection:   # src: A.2 of detcon paper
    type: "MLP" 
    input_dim: 768
    hidden_dim: 2048
    output_dim: 256
  predictor:
    type: "MLP"
    input_dim: 256
    hidden_dim: 4096
    output_dim: 256

clustering:
  scheduler:
    type: 'linear'
    epochs: [0,300]
    values: [128,4]
    # config = {
    #     "type":"cosine",
    #     "epochs":[0,300],
    #     "values":[64,1] # [1,64] work as well
    # }
    # # config = {
    # #     "type":"piecewise",
    # #     "epochs":[0,150,300],
    # #     "values":[1,64,32] # 0-150:1 ,150-300:64,300-inf:32
    # # }
    # # config = {
    # #     "type":"linear",
    # #     "epochs":[0,150,300],
    # #     "values":[1,64,32]
    # # }
amp:
  sync_bn: True
  opt_level: "O0"

data:
  image_dir: "/projects/d001/gce50852/kos/data/imagenet" #TODO: Change to match Japan Cluster
  subset: "imagenet1p" #Set to imagenet100 for imagenet100 or imagenet1p for 1p. "" = full imagenet
  mask_type: ""
  resize_size: 224 # src: 3.1
  data_workers: 1
  overlap_indicator: false 
  weight: true
  n_kmeans: 9999
  slic_segments: 100
  train_batch_size: 16 # src: A.3 (Global should be 4096 = batch_size x num_gpu)
  val_batch_size: 32 #  Should not matter
  dual_views: true
  over_lap_mask: false
  num_examples: 1281167 #For imagenet1p, imagenet100, byol_trainer automatically updates num_examples accordingly

optimizer:
  type: lars
  base_lr: 0.3
  lr_type: "cosine"
  momentum: 0.9 # src: Deepmind code config
  weight_decay: 1.0e-6
  total_epochs: 300
  warmup_epochs: 3 # src: Deepmind code; should be 1/100 of total epoches
  exclude_bias_and_bn: true

loss: #src: 3.1
  temperature: 0.1
  mask_rois: 16

checkpoint:
  time_stamp:
  resume_path: 
  save_epoch: 1
  ckpt_path: "r2o-vit/ckpt/{}/{}_{}_{}.pth.tar"

log:
  log_step: 10
  log_dir:
  log_all: False

stage: "train"
distributed: True
seed: 0
