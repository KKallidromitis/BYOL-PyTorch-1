model:
  base_momentum: 0.99
  masknet: false
  backbone:
    type: "resnet50"
    pretrained: false
  projection:   # src: A.2 of detcon paper
    type: "MLP" 
    input_dim: 2048
    hidden_dim: 4096
    output_dim: 256
  predictor:
    type: "MLP"
    input_dim: 256
    hidden_dim: 4096
    output_dim: 256

clustering:
  superpixel:
  scheduler:
    type: 'linear'
    epochs: [0,30]
    values: [4,4]
    # config = {
    #     "type":"cosine",
    #     "epochs":[0,300],
    #     "values":[64,1] # [1,64] work as well
    # }
    # # config = {
    # #     "type":"piecewise",
    # #     "epochs":[0,150,300],
    # #     "values":[1,64,32] # 0-150:1 ,150-300:64,300-inf:32
    # # }
    # # config = {
    # #     "type":"linear",
    # #     "epochs":[0,150,300],
    # #     "values":[1,64,32]
    # # }
amp:
  sync_bn: True
  opt_level: "O0"

data:
  image_dir: "coco" #TODO: Change to match Japan Cluster
  #subset: "imagenet1p" #Set to imagenet100 for imagenet100 or imagenet1p for 1p. "" = full imagenet
  mask_type: "coco"
  resize_size: 224 # src: 3.1
  data_workers: 16
  overlap_indicator: true 
  weight: false
  n_kmeans: 9999
  slic_segments: 100
  train_batch_size: 32 # src: A.3 (Global should be 4096 = batch_size x num_gpu)
  val_batch_size: 32 #  Should not matter
  dual_views: true
  over_lap_mask: false
  num_examples: 117266 #For imagenet1p, imagenet100, byol_trainer automatically updates num_examples accordingly

optimizer:
  type: lars
  base_lr: 0.3
  lr_type: "cosine"
  momentum: 0.9 # src: Deepmind code config
  weight_decay: 1.0e-6
  total_epochs: 30
  warmup_epochs: 3 # src: Deepmind code; should be 1/100 of total epoches
  exclude_bias_and_bn: true

loss: #src: 3.1
  temperature: 0.1
  mask_rois: 16

checkpoint:
  time_stamp:
  resume_path:
  save_epoch: 10
  ckpt_path: "ckpt/detconb/{}/{}_{}_{}.pth.tar"

log:
  log_step: 10
  log_dir:
  log_all: False

stage: "train"
distributed: True
seed: 0
