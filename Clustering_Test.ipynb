{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2ddc18bbf0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing BatchedKmeans with data from Normal(0, 1) and Normal(10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Kmeans Code\n",
    "###\n",
    "def batched_distance_matrix(x,y):\n",
    "    \"\"\"\n",
    "    Batched L2 Norm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Tensor of shape (Batch_Size , A, Feature_Dimension) \n",
    "    y : Tensor of shape (Batch_Size , B, Feature_Dimension) \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    D_ij : torch.Tensor of shape (Batch_Size , A, B )\n",
    "    \"\"\"\n",
    "    N,A,D = x.shape\n",
    "    _,B,D = y.shape\n",
    "    x_i = x.view(N,A, 1, D)\n",
    "    y_j = y.view(N,1,B,D)\n",
    "    D_ij = ((x_i - y_j) ** 2).sum(-1)\n",
    "    return D_ij\n",
    "\n",
    "def k_means_pp_batched(x, n_clusters):\n",
    "    \"\"\"\n",
    "    K-Means++ initialization\n",
    "\n",
    "    Based on Scikit-Learn's implementation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.Tensor of shape (n_training_samples, n_features)\n",
    "    n_clusters : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    centroids : torch.Tensor of shape (n_clusters, n_features)\n",
    "    \"\"\"\n",
    "\n",
    "    n_batch,n_samples, n_features = x.shape\n",
    "\n",
    "    centroids = torch.zeros((n_batch,n_clusters, n_features), dtype=x.dtype, device=x.device)\n",
    "\n",
    "    n_local_trials = 2 + int(np.log(n_clusters))\n",
    "\n",
    "    initial_centroid_idx = torch.randint(low=0, high=n_samples, size=(n_batch,), device=x.device)\n",
    "    batch_eye =  torch.eye(n_batch, dtype=x.dtype, device=x.device)[...,None] # B X B X 1\n",
    "    centroids[:,0, :] = torch.diagonal( x[:,initial_centroid_idx]).T #equivalent to (batch_eye * torch.index_select(x,1,initial_centroid_idx)).sum(dim=1)\n",
    "\n",
    "    # dist_mat = distance_matrix(x=centroids[0, :].unsqueeze(0), y=x,\n",
    "    #                            x_norm=x_norm[initial_centroid_idx, :].unsqueeze(0), y_norm=x_norm) # 1 X N\n",
    "    dist_mat = batched_distance_matrix(centroids[:,0:1, :],x)\n",
    "    #current_potential = dist_mat.sum(1) # 1\n",
    "    current_potential = dist_mat.sum(-1) # n_batch X 1\n",
    "    #print(n_clusters)\n",
    "    for c in range(1, n_clusters):\n",
    "        rand_vals = torch.rand(n_batch,n_local_trials, device=x.device) * current_potential # n_local_trials * (n_batch X 1)->  n_batch * n_local_trials\n",
    "        candidate_ids = torch.searchsorted(torch.cumsum(dist_mat.squeeze(1), dim=-1), rand_vals)\n",
    "        torch.clamp_max(candidate_ids, dist_mat.size(-1) - 1, out=candidate_ids)\n",
    "\n",
    "        # distance_to_candidates = distance_matrix(x=x[candidate_ids, :], y=x,\n",
    "        #                                          x_norm=x_norm[candidate_ids, :], y_norm=x_norm)\n",
    "\n",
    "        # _x_ii =torch.einsum('bxyd,bx->bxyd',x[:,candidate_ids,:],batch_eye.squeeze(-1)).sum(dim=1) # x[:,candidate_ids,:]-> B X B X n_local_trials X DIM\n",
    "        _x_ii = torch.diagonal( x[:,candidate_ids]).permute(2,0,1)\n",
    "        distance_to_candidates = batched_distance_matrix(_x_ii,x)\n",
    "\n",
    "        distance_to_candidates = torch.where(dist_mat < distance_to_candidates, dist_mat, distance_to_candidates)\n",
    "        candidates_potential = distance_to_candidates.sum(-1)\n",
    "\n",
    "        best_candidate = candidates_potential.argmin(-1)\n",
    "        #breakpoint()\n",
    "        current_potential = candidates_potential[:,best_candidate].diag().view(n_batch,1)\n",
    "        #breakpoint()\n",
    "        #dist_mat = distance_to_candidates[best_candidate].unsqueeze(0) # 1 X N\n",
    "        dist_mat = torch.diagonal(distance_to_candidates[:,best_candidate],dim1=0,dim2=1).T.unsqueeze(1) # B X N\n",
    "        best_candidate = candidate_ids[:,best_candidate].diag()\n",
    "\n",
    "        #centroids[c, :] = x[best_candidate, :]\n",
    "        centroids[:,c, :] = torch.diagonal( x[:,best_candidate]).T\n",
    "    return centroids\n",
    "\n",
    "def do_batch_kmeans(x, K=10, Niter=10,init='K-means++',eps=1e-6):\n",
    "    \"\"\"Implements Lloyd's algorithm for the Euclidean metric.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.Tensor of shape (Batch_Size,Samples_per_Batch, n_features)\n",
    "    init : string, Initialization method, defualt to 'K-means++', and alternative input are considered as taking first K elements as initial centroid in each batch\n",
    "    eps: float, threshold for termination\n",
    "    Returns\n",
    "    -------\n",
    "    cl : torch.Tensor of shape (Batch_Size,Samples_per_Batch), Cluster Assignment as Long Tensor\n",
    "    c : torch.Tensor of shape (Batch_Size,K, n_features), Centroids\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    N,B, D = x.shape  # Batch_Size X Samples per Batch X Dim\n",
    "    if init == 'K-means++':\n",
    "        c = k_means_pp_batched(x,K)\n",
    "    else:\n",
    "        c = x[:,:K, :].clone()  # Simplistic initialization for the centroids\n",
    "\n",
    "    x_i = x.view(N,B, 1, D)  # (N, 1, D) samples\n",
    "    c_j = c.view(N,1, K, D) # (1, K, D) centroids\n",
    "    Ncl = torch.zeros(N,K,1, dtype=x.dtype, device=x.device)\n",
    "    _ones = torch.ones(N,B,1, dtype=x.dtype, device=x.device)\n",
    "    # K-means loop:\n",
    "    # - x  is the (N, D) point cloud,\n",
    "    # - cl is the (N,) vector of class labels\n",
    "    # - c  is the (K, D) cloud of cluster centroids\n",
    "    for i in range(Niter):\n",
    "\n",
    "        # E step: assign points to the closest cluster -------------------------\n",
    "        D_ij = ((x_i - c_j) ** 2).sum(-1,keepdim=True)  # (N,B,K) symbolic squared distances\n",
    "        cl = D_ij.argmin(dim=2).long()  # Points -> Nearest cluster\n",
    "        inertia = D_ij.min(dim=2,keepdim=True).values.sum() #torch.sum(D_ij.min(dim=-1))\n",
    "        #breakpoint()\n",
    "        if abs(inertia) < eps:\n",
    "                # self.labels_ = labels\n",
    "                # self.inertia_ = inertia\n",
    "                # break\n",
    "                return cl,inertia\n",
    "        # M step: update the centroids to the normalized cluster average: ------\n",
    "        # Compute the sum of points per cluster:\n",
    "        c.zero_()\n",
    "        c.scatter_add_(1, cl.repeat(1,1, D), x)\n",
    "        # Divide by the number of points per cluster:\n",
    "        #Ncl = torch.bincount(cl, minlength=K).type_as(c).view(K, 1)\n",
    "        Ncl.zero_()\n",
    "        Ncl.scatter_add_(1, cl,_ones)\n",
    "        c /= Ncl  # in-place division to compute the average\n",
    "\n",
    "    # if verbose:  # Fancy display -----------------------------------------------\n",
    "    #     if use_cuda:\n",
    "    #         torch.cuda.synchronize()\n",
    "    #     end = time.time()\n",
    "    #     print(\n",
    "    #         f\"K-means for the Euclidean metric with {N:,} points in dimension {D:,}, K = {K:,}:\"\n",
    "    #     )\n",
    "    #     print(\n",
    "    #         \"Timing for {} iterations: {:.5f}s = {} x {:.5f}s\\n\".format(\n",
    "    #             Niter, end - start, Niter, (end - start) / Niter\n",
    "    #         )\n",
    "    #     )\n",
    "\n",
    "    return cl.view(N,B), c\n",
    "\n",
    "class BatchwiseKMeans:\n",
    "\n",
    "    def __init__(self,n_kmeans=10, Niter=200,init='K-means++',eps=1e-6):\n",
    "        self.n_kmeans = n_kmeans\n",
    "        self.Niter = Niter\n",
    "        self.init = init\n",
    "        self.eps = eps\n",
    "\n",
    "    def fit_transform(self,x):\n",
    "        assert len(x.shape) == 3\n",
    "        return do_batch_kmeans(x, K=self.n_kmeans, Niter=self.Niter,init=self.init,eps=self.eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_clustering(kmeans, k=2):\n",
    "    if k != 2:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    d1 = torch.distributions.normal.Normal(0, 1, validate_args=None) #0, 1 gaussian\n",
    "    d2 = torch.distributions.normal.Normal(10,1,validate_args=None) #10,1 gaussian\n",
    "\n",
    "    # Get samples\n",
    "    x1 = d1.sample((25,)).unsqueeze(-1) # Samples from N(0,1)\n",
    "    x2 = d2.sample((25,)).unsqueeze(-1) #Samples from N(10,1)\n",
    "    X = torch.stack([x1, x2]) #All samples\n",
    "\n",
    "    # Kmeans Cluster\n",
    "    labels, centroids = kmeans.fit_transform(X)\n",
    "\n",
    "    # Evaluate cluster accuracy\n",
    "    gt1 = torch.stack([torch.tensor([0]*25), torch.tensor([1]*25)])\n",
    "    gt2 = torch.stack([torch.tensor([1]*25), torch.tensor([0]*25)])\n",
    "    acc = max((labels == gt1).sum(), (labels == gt2).sum()) / torch.numel(labels)\n",
    "    #print(f\"Accuracy {acc*100}%\")\n",
    "    return acc * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "kmeans = BatchwiseKMeans(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchedKmeans Average Accuracy\n",
      "tensor(59.7800)\n",
      "[tensor(52.), tensor(56.), tensor(60.0000), tensor(64.), tensor(50.), tensor(62.), tensor(62.), tensor(68.), tensor(52.), tensor(62.), tensor(54.0000), tensor(62.), tensor(50.), tensor(60.0000), tensor(66.), tensor(54.0000), tensor(54.0000), tensor(52.), tensor(80.), tensor(66.), tensor(54.0000), tensor(56.), tensor(60.0000), tensor(54.0000), tensor(66.), tensor(64.), tensor(54.0000), tensor(54.0000), tensor(60.0000), tensor(74.), tensor(52.), tensor(50.), tensor(50.), tensor(58.), tensor(58.), tensor(56.), tensor(66.), tensor(66.), tensor(56.), tensor(60.0000), tensor(60.0000), tensor(56.), tensor(68.), tensor(58.), tensor(56.), tensor(58.), tensor(60.0000), tensor(60.0000), tensor(58.), tensor(60.0000), tensor(52.), tensor(52.), tensor(54.0000), tensor(70.), tensor(62.), tensor(74.), tensor(62.), tensor(50.), tensor(62.), tensor(54.0000), tensor(62.), tensor(76.), tensor(54.0000), tensor(64.), tensor(56.), tensor(58.), tensor(60.0000), tensor(52.), tensor(56.), tensor(56.), tensor(72.), tensor(68.), tensor(56.), tensor(56.), tensor(66.), tensor(54.0000), tensor(64.), tensor(78.), tensor(76.), tensor(60.0000), tensor(64.), tensor(50.), tensor(54.0000), tensor(52.), tensor(56.), tensor(66.), tensor(72.), tensor(58.), tensor(62.), tensor(56.), tensor(56.), tensor(60.0000), tensor(72.), tensor(60.0000), tensor(56.), tensor(56.), tensor(74.), tensor(54.0000), tensor(54.0000), tensor(62.)]\n"
     ]
    }
   ],
   "source": [
    "T = 100\n",
    "accs = []\n",
    "for _ in range(T):\n",
    "    accs.append(test_clustering(kmeans, K))\n",
    "\n",
    "print(\"BatchedKmeans Average Accuracy\")\n",
    "print(sum(accs) / len(accs))\n",
    "print(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Scikit-Learn Kmeans with Same 2 Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKLEARN KMeans (same hp) Avg Acc\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "kmean_sklearn = KMeans(K, n_init=1, max_iter=200, tol=1e-6, random_state=0)\n",
    "accs = []\n",
    "for _ in range(T):\n",
    "    x1_np = np.random.normal(0,1,25)\n",
    "    x2_np = np.random.normal(10, 1, 25)\n",
    "    X_np = np.concatenate([x1_np, x2_np]).reshape(-1, 1)\n",
    "\n",
    "    kmeans_output = kmean_sklearn.fit(X_np)\n",
    "    pred = kmeans_output.labels_\n",
    "    acc = max(accuracy_score(np.array([0]*25  + [1]*25), pred), accuracy_score(np.array([1]*25 + [0] * 25), pred))\n",
    "    accs.append(acc  * 100)\n",
    "\n",
    "print(\"SKLEARN KMeans (same hp) Avg Acc\")\n",
    "print(sum(accs) / len(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detcon_kernel",
   "language": "python",
   "name": "detcon_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
